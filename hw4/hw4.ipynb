{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2aa13-7254-4c74-83bd-ba6cc841c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, utils, models, datasets\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb013f0c-0049-412c-99f8-ef53e754bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 100\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1\n",
    "LR = 1e-3\n",
    "WD = 1e-5\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf99af-8478-44e3-96eb-30f0653e8b18",
   "metadata": {},
   "source": [
    "### 讀取資料集 - [VOC](https://drive.google.com/drive/folders/1TrJjsoIZ3QWecvOLGKCSa4NBk3qnObin)\n",
    "- [VOCDetection](https://pytorch.org/vision/stable/generated/torchvision.datasets.VOCDetection.html)\n",
    "- https://medium.com/codex/implementing-r-cnn-object-detection-on-voc2012-with-pytorch-b05d3c623afe\n",
    "- ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f2d5d-59e2-43ab-b8f8-3662c9f6243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.labels = ['train', 'car', 'chair', 'pottedplant', 'horse', 'cat', 'cow', 'bus', 'bicycle', 'person', 'dog', 'tvmonitor',\n",
    "                       'bird', 'motorbike', 'boat', 'aeroplane', 'sofa', 'sheep', 'bottle', 'diningtable']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.dataset[index][0]\n",
    "        annotation = self.dataset[index][1]['annotation']\n",
    "        \n",
    "        # output\n",
    "        target = {'image_id': torch.Tensor([int(annotation['filename'][:-4])]), 'boxes':[], 'labels':[], 'area':[]}\n",
    "        for obj in annotation['object']:\n",
    "            #label\n",
    "            target['labels'].append(self.labels.index(obj['name']))\n",
    "\n",
    "            # 物件區域\n",
    "            xmin = int(obj['bndbox']['xmin'])\n",
    "            ymin = int(obj['bndbox']['ymin'])\n",
    "            xmax = int(obj['bndbox']['xmax'])\n",
    "            ymax = int(obj['bndbox']['ymax'])\n",
    "            target['boxes'].append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            # 面積大小                        \n",
    "            target[\"area\"].append((xmax - xmin) * (ymax - ymin))\n",
    "    \n",
    "        target[\"boxes\"] = torch.FloatTensor(target[\"boxes\"])\n",
    "        target[\"labels\"] = torch.LongTensor(target[\"labels\"])\n",
    "        target[\"area\"] = torch.FloatTensor(target[\"area\"])\n",
    "        \n",
    "        # 圖形長寬\n",
    "        # w, h, d = list(map(int, annotation['size'].values()))\n",
    "        target[\"masks\"] = torch.zeros((len(annotation['object']), 300, 300), dtype=torch.float32)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b830f8a-4b80-4eeb-a33f-c984377d2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc dataset前處理\n",
    "transform = transforms.Compose([transforms.PILToTensor(), transforms.ConvertImageDtype(torch.float), transforms.CenterCrop(300)])\n",
    "# VOC\n",
    "dir_path = '/git/Kaiyang/hw4data'\n",
    "voc_trainset = VOCDataset(datasets.VOCDetection(dir_path, year=\"2007\", image_set=\"train\", download=False, transform=transform))\n",
    "voc_valset = VOCDataset(datasets.VOCDetection(dir_path, year=\"2007\", image_set=\"val\", download=False, transform=transform))\n",
    "voc_testset = VOCDataset(datasets.VOCDetection(dir_path, year=\"2007\", image_set=\"test\", download=False, transform=transform))\n",
    "\n",
    "\n",
    "print(f'訓練資料: {len(voc_trainset)}筆')\n",
    "print(f'驗證資料: {len(voc_valset)}筆')\n",
    "print(f'測試資料: {len(voc_testset)}筆')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2d9b7-741d-4228-a8a2-bd536db7a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 蒐集labels\n",
    "# labels_list = []\n",
    "# for i, data in enumerate(voc_trainset):\n",
    "#     label = data[1]['annotation']['object'][0]['name']\n",
    "#     if label not in labels_list:\n",
    "#         labels_list.append(label)\n",
    "# print(labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af341227-c822-4577-a495-530424cb5b05",
   "metadata": {},
   "source": [
    "### 讀取資料集 - [ADE20K](https://drive.google.com/drive/folders/1hRy6am8KeUWW_6sgj46_Qk7_vbWDWhRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ddc22-9a72-4fe9-a00b-756c0f6fe5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ADEDataset(Dataset):\n",
    "#     def __init__(self, path, filename, transform):\n",
    "#         self.path = path\n",
    "#         self.image_list = self.read_image(os.path.join(path, filename))\n",
    "#         self.transform = transform\n",
    "    \n",
    "#     def read_image(self, filename):\n",
    "#         image_list = []\n",
    "#         with open(filename, 'r') as f:\n",
    "#             lines = f.readlines()\n",
    "#             for line in lines:\n",
    "#                 img = line.rstrip().split(' ')[0]\n",
    "#                 image_list.append(img)\n",
    "#         return image_list\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.path)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         # load images and masks\n",
    "#         image_name= self.image_list[idx]\n",
    "#         img_path = os.path.join(self.root, \"imgs\", f\"ADE_val_{image_name}.jpg\")\n",
    "#         img = Image.open(img_path).convert(\"RGB\")\n",
    "#         json_path = os.path.join(self.root, \"jsons\", f\"ADE_val_{image_name}.json\")\n",
    "#         mask_path = os.path.join(self.root, \"instance_mask_backup\")\n",
    "        \n",
    "#         with open(json_path) as f:\n",
    "#             img_data = json.load(f)\n",
    "\n",
    "#         label = []\n",
    "#         masks = []\n",
    "#         boxes = []\n",
    "#         for obj in img_data['annotation']['object']:\n",
    "#             id = obj['id']\n",
    "#             obj_name = obj['name'].split(\",\")\n",
    "#             for single_name in obj_name:    \n",
    "#                 single_name = single_name.strip() \n",
    "#                 if  single_name in classes.keys():\n",
    "#                     xmin = min(obj['polygon']['x'])\n",
    "#                     xmax = max(obj['polygon']['x'])\n",
    "#                     ymin = min(obj['polygon']['y'])\n",
    "#                     ymax = max(obj['polygon']['y'])\n",
    "#                     if xmin == xmax or ymin == ymax:\n",
    "#                         break\n",
    "                        \n",
    "#                     boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    \n",
    "#                     label.append(classes[single_name])\n",
    "\n",
    "#                     instance_path = os.path.join(mask_path, obj[\"instance_mask\"])\n",
    "#                     mask = np.array(Image.open(instance_path))\n",
    "#                     masks.append(mask)\n",
    "#                     break\n",
    "\n",
    "#         boxes = np.array(boxes)\n",
    "#         # convert everything into a torch.Tensor\n",
    "#         area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "#         # suppose all instances are not crowd\n",
    "\n",
    "#         target = {}\n",
    "#         target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "#         target[\"labels\"] = torch.as_tensor(np.array(label), dtype=torch.int64) - 1\n",
    "#         target[\"masks\"] = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "#         target[\"image_id\"] = torch.tensor([idx])\n",
    "#         target[\"area\"] =torch.as_tensor(area, dtype=torch.uint8)\n",
    "#         target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "\n",
    "#         if self.transforms is not None:\n",
    "#             img, target = self.transforms(img, target)\n",
    "\n",
    "#         return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514fd94-707f-470c-a271-9483b0769b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ade_trainset = ADEDataset(dir_path, 'train.txt', transform=transform)\n",
    "# ade_valset = ADEDataset(dir_path, 'val.txt', transform=transform)\n",
    "# ade_testset = ADEDataset(dir_path, 'test.txt', transform=transform)\n",
    "\n",
    "# print(f'訓練資料: {len(ade_trainset)}筆')\n",
    "# print(f'驗證資料: {len(ade_valset)}筆')\n",
    "# print(f'測試資料: {len(ade_testset)}筆')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efb569-b061-4401-af17-4306445fbfdf",
   "metadata": {},
   "source": [
    "### set DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd74de3-5d71-49ba-84dd-6c31b17c9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     images = [item[0] for item in batch]\n",
    "#     targets = [item[1] for item in batch]\n",
    "\n",
    "#     imgs = []\n",
    "#     for image in images:\n",
    "#         imgs.append(image)\n",
    "\n",
    "#     bndboxes = [target[\"boxes\"] for target in targets]\n",
    "#     labels = [target[\"labels\"] for target in targets]\n",
    "#     image_ids = [target[\"id\"] for target in targets]\n",
    "#     areas = [target[\"bndbox\"] for target in targets]\n",
    "#     masks = [target[\"masks\"] for target in targets]\n",
    "\n",
    "#     tars = []\n",
    "#     for i in range(len(batch)):\n",
    "#         box = bndboxes[i]\n",
    "#         label = labels[i]\n",
    "#         image_id = image_ids[i]\n",
    "#         area = areas[i]\n",
    "#         mask = masks[i] \n",
    "#         target = {\"image_id\": image_id, \"labels\": label, \"boxes\": box, \"area\": area, \"masks\": mask}\n",
    "#         tars.append(target)\n",
    "\n",
    "#     return imgs, tars\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(voc_trainset, batch_size=BATCH_SIZE, shuffle = True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(voc_valset, batch_size=BATCH_SIZE, shuffle = False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(voc_testset, batch_size=BATCH_SIZE, shuffle = False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c59a7-e9c6-437a-a566-55b3a098b2be",
   "metadata": {},
   "source": [
    "### Dual Task\n",
    "- 物件檢測(Object Detection): 目標為檢測出圖像中所有的物體，並且給出物體的位置和邊界框\n",
    "- 語義分割(Semantic Segmentation): 目標為將圖像中的每個像素分配到相應的語義類別中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119004aa-e91c-494f-bb7b-3a8bdec51a30",
   "metadata": {},
   "source": [
    "### [Mask_RCNN](https://github.com/matterport/Mask_RCNN)\n",
    "- Faster R-CNN的two-stage模型加上Feature Pyramid Network(FPN)的方法，利用不同維度下特徵層級高的feature maps來進行預測\n",
    "- 改良Faster R-CNN中ROI Pooling的缺點，使其邊界框和物體定位的經度可以真正達到像素等級\n",
    "- [參考網址](https://ivan-eng-murmur.medium.com/%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-s9-mask-r-cnn-%E7%B0%A1%E4%BB%8B-99370c98de28)\n",
    "- [如何使用MRCNN](https://github.com/sagieppel/Train_Mask-RCNN-for-object-detection-in_In_60_Lines-of-Code/blob/main/test.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0f30c-85c8-46d2-b015-c2b74257821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pre-trained MRCNN\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) \n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features \n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=20)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# set the optimizer\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "# show model architecture\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c7ef0-d055-404a-947c-944cff680dbb",
   "metadata": {},
   "source": [
    "### train - Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1f27f-0ac2-49f3-8b05-894061ab9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer():\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, epochs, optimizer):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def train(self):\n",
    "        self.train_loss_history = []\n",
    "        self.valid_acc_history = []\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = 0.0\n",
    "            for i, data in enumerate(self.train_loader):\n",
    "                images, targets = data\n",
    "                images = list(image.to(self.device) for image in images)\n",
    "                targets = [{key: value.to(self.device) for key, value in t.items()} for t in targets]\n",
    "                # Zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # model forward\n",
    "                loss_dict = self.model(images, targets)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                losses.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += losses.item()\n",
    "            \n",
    "            # valid data\n",
    "            self.model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, data in enumerate(self.val_loader):\n",
    "                # read data\n",
    "                images, targets = data\n",
    "                images = list(image.to(self.device) for image in images)\n",
    "                targets=[{key: value.to(self.device) for key, value in t.items()} for t in targets]\n",
    "                \n",
    "                # model forward\n",
    "                outputs = self.model(images)\n",
    "                outputs = [{key: value.to('cpu') for key, value in t.items()} for t in outputs]\n",
    "                labels = targets[0]['labels'].to('cpu').numpy()\n",
    "                outputs = outputs[0]['labels'].to('cpu').numpy()\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i] == outputs[i]:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "                    \n",
    "            train_loss = train_loss / len(self.train_loader)\n",
    "            val_acc = correct / total\n",
    "            self.train_loss_history.append(train_loss)\n",
    "            print(f\"--------------------Epoch {epoch+1}--------------------\")\n",
    "            print(f\"Train_loss: {train_loss:.3f} | Val_acc: {val_acc:.3f}\")\n",
    "            \n",
    "        # print the curves of the training loss and validation loss\n",
    "        self.plot_loss()\n",
    "    \n",
    "    def predict(self):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(self.test_loader):\n",
    "            # read data\n",
    "            images, targets = data\n",
    "            images = list(image.to(self.device) for image in images)\n",
    "            targets=[{key: value.to(self.device) for key, value in t.items()} for t in targets]\n",
    "\n",
    "            # model forward\n",
    "            outputs = self.model(images)\n",
    "            outputs = [{key: value.to('cpu') for key, value in t.items()} for t in outputs]\n",
    "            labels = targets[0]['labels'].to('cpu').numpy()\n",
    "            outputs = outputs[0]['labels'].to('cpu').numpy()\n",
    "            for i in range(len(labels)):\n",
    "                if labels[i] == outputs[i]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "                \n",
    "        acc = correct / total\n",
    "        print(f\"----------------------------------------------------\")\n",
    "        print(f'Test acc: {acc:.3f}')\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.train_loss_history)\n",
    "        plt.title('Loss History')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['train'], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493eaee8-d63b-4638-af20-bd0edcc90c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrcnn_trainer = trainer(model, train_loader, val_loader, test_loader, EPOCHS, optimizer)\n",
    "mrcnn_trainer.train()\n",
    "mrcnn_trainer.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d5334-8dbb-48da-ac26-6991b2a49a0e",
   "metadata": {},
   "source": [
    "### train - Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4e928-5ab1-4f73-95f4-3a9eb5710d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2 = DataLoader(ade_trainset, batch_size=BATCH_SIZE, shuffle = True, collate_fn=collate_fn)\n",
    "val_loader2 = DataLoader(ade_valset, batch_size=BATCH_SIZE, shuffle = False, collate_fn=collate_fn)\n",
    "test_loader2 = DataLoader(ade_testset, batch_size=BATCH_SIZE, shuffle = False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01825f7-074d-4188-bcb8-968603330fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mrcnn_trainer2 = trainer(model, train_loader2, val_loader2, test_loader2, EPOCHS, optimizer)\n",
    "# mrcnn_trainer2.train()\n",
    "# mrcnn_trainer2.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
